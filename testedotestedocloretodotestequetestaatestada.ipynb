{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRJGUg4sA2PzX8lbVCdzgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardaVelascoCatanho/cloreto/blob/main/testedotestedocloretodotestequetestaatestada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "G_AaHn1FTKhH"
      },
      "outputs": [],
      "source": [
        "!pip -q install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "_RHDyjjyeSst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "MODEL_ID = \"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "2dZfBzFs20C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "D-EL-KAs3GEI",
        "outputId": "29afe7c4-28c8-4754-e6b4-a0c6281a3e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response:\n A Alura n√£o tem uma data espec√≠fica para a pr√≥xima Imers√£o IA com o Google Gemini no momento. A melhor forma de saber quando uma nova edi√ß√£o ser√° lan√ßada √©:\n\n*   **Acompanhar a p√°gina da Alura:** Fique de olho na p√°gina da Alura dedicada a Imers√µes e Cursos de Intelig√™ncia Artificial.\n*   **Assinar a Newsletter da Alura:** Cadastre seu e-mail para receber novidades sobre os cursos e imers√µes da Alura.\n*   **Seguir a Alura nas Redes Sociais:** A Alura costuma divulgar seus lan√ßamentos no Instagram, LinkedIn, e outras plataformas."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalar o Agent Development Kit\n",
        "\n",
        "!pip -q install google-adk"
      ],
      "metadata": {
        "id": "AxQjS4tar2sH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.adk.agents import Agent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.tools import google_search\n",
        "from google.genai import types #Isso vai criar conte√∫do tipo ocorreu na aula 4\n",
        "from datetime import date\n",
        "import textwrap #Formatar a sa√≠da de texto\n",
        "from IPython.display import display, Markdown #Exibir texto formatado no Google Colab\n",
        "import requests #Para requisi√ß√µes HTTP (veja se pode limitar isso a HTTPS)\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "vWdUa4sCsN2h"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wJoZN7Vftp5",
        "outputId": "d6b410d8-f9fd-455b-c588-e4f634afbcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = 'gemini-2.0-flash'\n",
        "\n",
        "resposta = client.models.generate_content(model=modelo,\n",
        "                                          contents=\"Quem √© bl√° bl√° por tr√°s do Gemini?\")\n"
      ],
      "metadata": {
        "id": "6z79Vd27f8sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "qJbyjn4ChSIR",
        "outputId": "887abdd1-b785-415e-8a67-71c15ef8fcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O Gemini √© um modelo de linguagem grande (LLM) desenvolvido pelo Google AI. A equipe por tr√°s do Gemini √© composta por diversos pesquisadores, engenheiros e especialistas em intelig√™ncia artificial.\\n\\nEmbora n√£o haja uma √∫nica pessoa que possa ser apontada como \"o criador\" do Gemini, algumas figuras-chave do Google AI t√™m um papel importante no desenvolvimento de modelos de linguagem como o Gemini:\\n\\n*   **Equipe Google AI:** A equipe como um todo √© respons√°vel pela pesquisa, desenvolvimento e treinamento do modelo.\\n*   **Sundar Pichai:** Como CEO da Alphabet (empresa-m√£e do Google), ele supervisiona as iniciativas de intelig√™ncia artificial da empresa.\\n*   **Jeff Dean:** Cientista-chefe do Google, lidera muitas das iniciativas de IA da empresa.\\n*   **Demis Hassabis:** CEO e cofundador do Google DeepMind, uma unidade do Google focada em IA.\\n\\n√â importante notar que o desenvolvimento de um modelo como o Gemini √© um esfor√ßo colaborativo que envolve muitas pessoas com diferentes √°reas de especializa√ß√£o.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=modelo)\n",
        "resposta = chat.send_message('Oi, tudo bem?')\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CqI8-CD2hVmb",
        "outputId": "1af6d8e1-0d3f-4574-c53c-876cfdd7bab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tudo bem por aqui! üòä Como posso te ajudar hoje?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message('Voc√™ √© um assistente pessoal e voc√™ sempre responde de forma sucinta.')\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k7Kc5oH7LUHt",
        "outputId": "6113fa10-1e2b-4c28-f6ad-b8d5b3beb639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OK. O que precisa?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "chat_config = types.GenerateContentConfig(\n",
        "    system_instruction='Voc√™ √© um assistente pessoal e voc√™ sempre responde de forma sucinta'\n",
        "\n",
        ")\n",
        "\n",
        "chat = client.chats.create(model=modelo, config=chat_config)"
      ],
      "metadata": {
        "id": "cCBaruMKP__9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message('O que √© computa√ß√£o qu√¢ntica')\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XMGpBCyYaPF4",
        "outputId": "44bc4218-3fbd-400d-c8fb-fecd598ffd8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Computa√ß√£o qu√¢ntica √© um tipo de computa√ß√£o que usa fen√¥menos da mec√¢nica qu√¢ntica, como superposi√ß√£o e entrela√ßamento qu√¢ntico, para realizar opera√ß√µes em dados.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "FVlAF-kbhSFo",
        "outputId": "378859e2-9bf4-4d8c-87ce-b0e4bbb5f80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method _BaseChat.get_history of <google.genai.chats.Chat object at 0x7d50cde7b750>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>google.genai.chats._BaseChat.get_history</b><br/>def get_history(curated: bool=False) -&gt; list[Content]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/google/genai/chats.py</a>Returns the chat history.\n",
              "\n",
              "Args:\n",
              "    curated: A boolean flag indicating whether to return the curated (valid)\n",
              "      history or the comprehensive (all turns) history. Defaults to False\n",
              "      (returns the comprehensive history).\n",
              "\n",
              "Returns:\n",
              "    A list of `Content` objects representing the chat history.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 178);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pergunte ao Gemini uma informa√ß√£o mais recente que seu conhecimento\n",
        "\n",
        "from IPython.display import HTML, Markdown\n",
        "\n",
        "#Pergunta gen√©rica\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='Quando √© a pr√≥xima Imers√£o IA com o Google Gemini da Alura?'\n",
        ")\n",
        "\n",
        "display(Markdown(f\"Response:\\n {response.text}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "eOFrxLAuJBIt",
        "outputId": "03e7aa66-423d-4b75-feca-9f7323908fd5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Response:\n A Alura n√£o tem uma data fixa para a Imers√£o IA com Google Gemini. A melhor forma de saber quando a pr√≥xima edi√ß√£o acontecer√° √©:\n\n*   **Acompanhar as redes sociais e o site da Alura:** A Alura geralmente anuncia novas edi√ß√µes de seus cursos e imers√µes em suas redes sociais (como LinkedIn, Instagram, Twitter/X) e no site oficial.\n*   **Assinar a newsletter da Alura:** Ao assinar a newsletter, voc√™ receber√° informa√ß√µes sobre os pr√≥ximos cursos e eventos diretamente no seu e-mail.\n*   **Verificar a p√°gina da Imers√£o IA com Google Gemini no site da Alura:** Mesmo que n√£o haja uma data definida, a p√°gina do curso pode ter informa√ß√µes sobre a possibilidade de novas edi√ß√µes e um formul√°rio para voc√™ demonstrar seu interesse.\n\nFazendo isso, voc√™ ser√° um dos primeiros a saber quando a pr√≥xima Imers√£o IA com Google Gemini ser√° realizada!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Esperando prompt: \")\n",
        "\n",
        "while prompt != 'fim':\n",
        "  resposta = chat.send_message(prompt)\n",
        "  print(\"Resposta: \", resposta.text)\n",
        "  print(\"\\n\")\n",
        "  prompt = input(\"Esperando prompt: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg88PHcyfIlh",
        "outputId": "07df836a-76fa-464e-f4ab-6eccde898fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esperando prompt: oi\n",
            "Resposta:  Ol√°! Em que posso ajudar hoje?\n",
            "\n",
            "\n",
            "\n",
            "Esperando prompt: fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tH6GUkV65Thc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}